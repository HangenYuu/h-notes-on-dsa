[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nThis blog is mostly for my own study of data structures and algorithms and LeetCode questions. Therefore, it lacks the normal structure of a study resource for audience, especially for the LeetCode questions mentioned. There will be no problem statements, constraints, etc., only notes on intuition, approach, and code solutions with comments. Please open the corresponding LeetCode/LintCode questions while reading."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "H’s notes on Data Structuress and Algorithms",
    "section": "",
    "text": "Array\n\n\n\n\n\nArray\n\n\n\n\n\n\nMay 7, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo pointers\n\n\n\n\n\nTwo pointers\n\n\n\n\n\n\nMay 4, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\n  \n\n\n\n\nTree\n\n\n\n\n\nMostly binary tree\n\n\n\n\n\n\nMay 4, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStack\n\n\n\n\n\nStack\n\n\n\n\n\n\nMay 4, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSliding window\n\n\n\n\n\nA variation of two pointers\n\n\n\n\n\n\nMay 4, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary search\n\n\n\n\n\nBinary search\n\n\n\n\n\n\nMay 4, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraph\n\n\n\n\n\nGraph\n\n\n\n\n\n\nMay 4, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinked List\n\n\n\n\n\nLinked List\n\n\n\n\n\n\nMay 4, 2023\n\n\nPham Nguyen Hung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/array/index.html",
    "href": "posts/array/index.html",
    "title": "Array",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/array/index.html#definition",
    "href": "posts/array/index.html#definition",
    "title": "Array",
    "section": "Definition:",
    "text": "Definition:\nThe easiest, linear data structure, consisting of elements stored in contiguous memory slots. Elements are access by index, usually a number. The dictionary, or HashMap is a related data structure, consisting of key-value pairs that will be used in dealing array question."
  },
  {
    "objectID": "posts/array/index.html#problem",
    "href": "posts/array/index.html#problem",
    "title": "Array",
    "section": "Problem:",
    "text": "Problem:\n\n1. Two Sum:\nThe basic of basic problem, the first problem everybody encounters when he/she starts LeetCoding.\n\nIntuition\n\nTraversing the list seems like the most logical thing to do. The brute-force way is to check every pair of numbers. The time complexity is O(n^2). To optimize this, it is better to store the information we have already encountered when we traverse the list. We need to use a data structure that supports fast searching - HashMap. Either way works, but the HashMap can be used for storing the index of the element as well. If we store an element as the key and the index as the value, we can quickly search and return the indices.\n\n\nApproach\n\nThe algorithm can be described as:\n\nTraverse the array from the beginning.\nStore the element encountered in a set/dictionary.\nCheck if the sum - element is in the dictionary.\nReturn the indices.\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): Traversing the whole list once.\nSpace complexity:  \\(O(n)\\): In the worst case, we will have a dictionary with a size equal to the array.\n\n\n\nCode\nfrom collections import defaultdict\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n        numsDict = defaultdict(int)\n        for ind, num in enumerate(nums):\n            if target-num in numsDict:\n                return [ind, numsDict[target-num]]\n            numsDict[num] = ind\n\n\n\n2. Contains Duplicate\n\nIntuition\n\nAgain, traverse the array, storing information about what we have encountered. This time, we only need to store the element itself, so a HashSet suffices.\n\n\nApproach\n\n\nTraverse the array.\nCheck if the current element is in the HashSet. If it is, return True for duplicate.\nIf we reach the end of the array, return False for no duplicate.\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): Traversing the whole list once.\nSpace complexity:  \\(O(n)\\): In the worst case, we will have a set with a size equal to the array.\n\n\n\nCode\nclass Solution:\n    def containsDuplicate(self, nums: List[int]) -> bool:\n        numSet = set()\n        for num in nums:\n            if num in numSet:\n                return True\n            numSet.add(num)\n        return False\n\n\n\n3. Valid Anagram:\n\nIntuition\n\nAgain, traverse the array, storing information about what we have encountered. This time, we only need to store the element itself, so a HashSet suffices.\n\n\nApproach\n\n\nTraverse the array.\nCheck if the current element is in the HashSet. If it is, return True for duplicate.\nIf we reach the end of the array, return False for no duplicate.\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): Traversing the whole list once.\nSpace complexity:  \\(O(n)\\): In the worst case, we will have a set with a size equal to the array.\n\n\n\nCode\nfrom collections import defaultdict\nclass Solution:\n    def isAnagram(self, s: str, t: str) -> bool:\n        if len(s) != len(t):\n            return False\n        hashS, hashT = defaultdict(int), defaultdict(int)\n        for index in range(len(s)):\n            hashS[s[index]] += 1\n            hashT[t[index]] += 1\n        return hashS == hashT\n\n\n\n4. Majority Element\n\nIntuition\n\nThere are many approaches to the problem. The simplest way except brute-force the whole thing is to use a HashMap to count the number of occurences of each element (in Python can be done faster with collections.Counter). The solution provided is to deal with the follow-up question of linear time with \\(O(1)\\) memory.\n\n\nApproach\n\nThis is a named algorithm: Boyer-Moore Voting Algorithm. The procedure is this: while we traverse the array, if we choose the first element as a candidate and consider three things:\n\nAn encounter with the same element a vote for the candidate\nAn encounter with a difference element a vote against the candidate\nChange the candidate to the current element if it is different than the candidate and the vote for the candidate has become 0\n\nthen at the end, we should be left with the candidate.\nThis is because the majority element will occur more than n//2 times with n the length of the array. Because of this, the majority element will be the only possible candidate with a positive vote at the end.\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): We need to traverse the array once.\nSpace complexity:  \\(O(1)\\): We only need to keep track of the count and the candidate, which requires constant memory.\n\n\n\nCode\nclass Solution:\n    def majorityElement(self, nums):\n        count = 0\n        candidate = None\n\n        for num in nums:\n            if count == 0:\n                candidate = num\n            count += (1 if num == candidate else -1)\n\n        return candidate\n\n\n\n5. Group Anagrams\n\nIntuition\nFrom Valid Anagram, we know that one word is an anagram of another if character frequencies are the same. We can check everything in one go with a dictionary. The key information is lowercase letter only. This means that character frequency can be capture in a tuple of 26 number, each one the number of character a, b, etc. in the word. We can append each word to the correct key in the dictionary. Afterwards, we simply return the values of the dictionary.\n\n\nApproach\n\nInitialize a dictionary, better with Python defaultdict.\nIterate the list of words. For each word, initialize an array of 26 0s and increment the correct count for each character. ord() is useful in this case.\nAppend the word to the value array of the correct key.\nReturn the values of the dictionary.\n\n\n\nComplexity\n\nTime complexity:  \\(O(nk)\\): We need to traverse the array once. \\(k\\) is the maximum length of a string in the array.\nSpace complexity:  \\(O(nk)\\): The information stored in the dictionary.\n\n\n\nCode\nfrom collections import defaultdict\nclass Solution:\n    def groupAnagrams(self, strs: List[str]) -> List[List[str]]:\n        countDict = defaultdict(list)\n        for s in strs:\n            count = [0] * 26\n            for c in s:\n                count[ord(c) - ord('a')] += 1\n            countDict[tuple(count)].append(s)\n        return countDict.values()\n\n\n\n6. Top K Frequent Elements\n\nIntuition\nTo satisfy the \\(O(nlogn)\\) time constraint, computer science students will think of Quickselect. If you know how to implement Quickselect, it’s fine.\nFor someone who doesn’t, the approach is to use a dictionary the frequency as the key, and the value an array of elements with that key. To return, we can just iterate the dictionary from the maximum value to the minimum, append the value to an array and return the k elements. #### Approach\n\nTake frequency count of the elements in the array, better with Python Counter\nInitialize a dictionary, better with Python defaultdict.\nIterate the array. Append each element to the value of the correct key.\nIterate the dictionary from the maximum value down for the top-k elements and return it.\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): We need to traverse the array once and the dictionary once. Worst-case, the dictionary has the same size as the array.\nSpace complexity:  \\(O(n)\\): Worst-case, we will need to store triple the memory of the original array.\n\n\n\nCode\nfrom collections import defaultdict, Counter\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n        count = Counter(nums)\n        ans = defaultdict(list)\n        for num in count:\n            ans[count[num]].append(num)\n        top = max(ans)\n        res = []\n        for i in range(top,-1,-1):\n            res.extend(ans[i])\n        return res[:k]\nOptimized solution from NeetCode\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n        # Bucket Sort\n        countDict = {}\n        for i in nums:\n            countDict[i] = countDict.get(i, 0) + 1\n        freqList = [[] for i in range(len(nums))]\n        for num, count in countDict.items():\n            freqList[count-1].append(num)\n        res = []\n        for i in range(len(freqList) - 1, 0, -1):\n            for n in freqList[i]:\n                res.append(n)\n                if len(res) == k:\n                    return res\n\n\n\n7. Encode and Decode Strings\n\nIntuition\nThere are many approaches to this problem. My approach is the simplest: I will use an usual combination of character to insert between strings in the list. To minimize memory use, I choose a 3 combo: '*-*'. I reckon that I can get the work done with '*-', but '*-* is safer and cuter.\nA more seasoned programmer will give a more complex approach. He will use ':' to insert between, and inserting '::' when there is a ':'. This is a general method, which takes care of the case you need to impress the guys by showing that you can think generally, given that you may need to learn another language at the company. But in Python, simpler is better - it is just an one-liner with fast, optimized method for both functions. But of course, it is not so impressive, making you look so dependent on Python.\nNeetCode has a middle approach - Use the length and '#' to insert before each character. This is much better.\n\n\nApproach\nOne-liner: Use built-in method .join() and .split() to do the heavy work.\nGeneral:\n\nEncode the string by concatenating the length + '#' + the string into a big one and return.\nDecode the string by creating a for loop and deal with phrases consequentially.\n\nExtract the character length of the next phrase from the string.\nUse slicing to get the phrase and append to the result list.\nMove on to the next position.\nReturn the result.\n\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): We need to traverse the array and string once for each.\nSpace complexity:  \\(O(1)\\): We will need to store some pointers for both functions.\n\n\n\nCode\nOne-liner\nclass Solution:\n    \"\"\"\n    @param: strs: a list of strings\n    @return: encodes a list of strings to a single string.\n    \"\"\"\n    def encode(self, strs):\n        # write your code here\n        return '*-*'.join(strs)\n\n    \"\"\"\n    @param: str: A string\n    @return: dcodes a single string to a list of strings\n    \"\"\"\n    def decode(self, str):\n        # write your code here\n        return str.split('*-*')\nOptimized NeetCode\nclass Solution:\n    \"\"\"\n    @param: strs: a list of strings\n    @return: encodes a list of strings to a single string.\n    \"\"\"\n\n    def encode(self, strs):\n        res = \"\"\n        for s in strs:\n            res += str(len(s)) + \"#\" + s\n        return res\n\n    \"\"\"\n    @param: s: A string\n    @return: decodes a single string to a list of strings\n    \"\"\"\n\n    def decode(self, s):\n        res, i = [], 0\n\n        while i < len(s):\n            j = i\n            while s[j] != \"#\":\n                j += 1\n            length = int(s[i:j])\n            res.append(s[j + 1 : j + 1 + length])\n            i = j + 1 + length\n        return res\n\n\n\n8. Product of Array Except Self\n\nIntuiton\nThe trick is to use “prefix” and “postfix” arrays, respectively storing the product of all elements coming before and after the current values. These arrays have the properties of being able to be calculated iteratively in a for loop from the calculated value before. The answer is simply the element-wise product of the two arrays.\nTo get to the \\(O(1)\\) memory solution, it is about converting one array into just a pointer to cache the product.\n\n\n\nApproach\n\nTraverse the array to calculate the prefix (postfix array).\nTraverse the array again to update the postfix product and also the result.\nReturn the result.\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): We need to traverse the original array twice.\nSpace complexity:  \\(O(1)\\): We will need to store the postfix product while calculating.\n\n\n\nCode\nclass Solution:\n    def productExceptSelf(self, nums: List[int]) -> List[int]:\n        # To achieve O(1) space, will need to perform the multiplication \n        # directly on the prefix array\n        # postfix array needs to become just a pointer to the cached product \n        lenNum = len(nums)\n        prefix, postfix = [1] * lenNum, 1\n        for i in range(1, lenNum):\n            prefix[i] = prefix[i-1]*nums[i-1]\n        for i in range(lenNum-1, -1, -1):\n            prefix[i] *= postfix\n            postfix *= nums[i]\n        return prefix"
  },
  {
    "objectID": "posts/binary_search/index.html",
    "href": "posts/binary_search/index.html",
    "title": "Binary search",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/binary_search/index.html#definitions",
    "href": "posts/binary_search/index.html#definitions",
    "title": "Binary search",
    "section": "Definitions:",
    "text": "Definitions:\nIn a sorted array, there exists a faster way to search for a specific element than visiting each element. The intuition is if we find an element that is smaller than expected, all of the elements before it are also smaller and can be discarded from the search and likewise if it is larger. The optimal way to take advantage of this property is striking in the middle every time, hence the name binary search."
  },
  {
    "objectID": "posts/binary_search/index.html#problem",
    "href": "posts/binary_search/index.html#problem",
    "title": "Binary search",
    "section": "Problem",
    "text": "Problem\n\n1. Binary serach\n\nIntuition\n\nThe basic problem of binary search. It basically asks you to implement it.\n\n\nApproach\n\n\nInitialize two pointers - start and end.\nTraverse the array from both ends.\nCalculate middle = (start + end)//2 and compare array[middle] with the target.\nIf we found the target, return middle. If we are smaller, move start to middle. If we are larger, move end to middle.\nIf we reach the end, return -1 as we do not find the target.\n\n\n\nComplexity\n\nTime complexity:  \\(O(logn)\\): We are cutting the array in half repeatedly, so it takes just \\(logn\\) to search.\nSpace complexity:  \\(O(1)\\): Pointers are essentially integers.\n\n\n\nCode\nclass Solution:\n    def search(self, nums: List[int], target: int) -> int:\n        # Edge case\n        if nums[0] > target or nums[-1] < target:\n            return -1\n        \n        # General case\n        if nums[0] == target:\n            return 0\n        if nums[-1] == target:\n            return len(nums) - 1\n        start, end = 0, len(nums) - 1\n        while start <= end:\n            mid = (start + end)//2\n            if nums[mid] == target:\n                return mid\n            elif nums[mid] < target:\n                start = mid + 1\n            else:\n                end = mid - 1\n        \n        return -1\n\n\n\n2. First Bad Version\n\nIntuition\n\nFor any call isBadVersion(version), if the version is good, it means that the first bad version will be a later version; if the version is bad, it means that the first bad version might be an older version. And given the version comes in non-decreasing order, that sounds exactly like a binary search problem. Notice the italic will be and might be - it will affect the way we implement the algorithm.\n\n\nApproach\n\n\nInitialize two pointers - first and last or whatever to the first and the last version, denoted by numbers.\nThe terminating condition for this specific implementation is first == last, so the loop condition is while first < last.\nWe will check the mid version. If it is bad, we will shift last to mid. If it is good, we will shift first to mid + 1.\nReturn first (or last - does not matter) when the loop terminates. Cannot return mid here as the variable does not exist outside the loop. You can define it so, but there’s no need to.\n\n\n\nComplexity\n\nTime complexity:  \\(O(log(n))\\): The default time complexity of binary search.\nSpace complexity:  \\(O(1)\\): Pointers are essentially integers.\n\n\n\nCode\n# The isBadVersion API is already defined for you.\n# def isBadVersion(version: int) -> bool:\n\nclass Solution:\n    def firstBadVersion(self, n: int) -> int:\n        low, high = 1, n\n        while low < high:\n            mid = (low+high)//2\n            if isBadVersion(mid):\n                high = mid\n            else:\n                low = mid + 1\n        return low #high"
  },
  {
    "objectID": "posts/graph/index.html",
    "href": "posts/graph/index.html",
    "title": "Graph",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/graph/index.html#definitions",
    "href": "posts/graph/index.html#definitions",
    "title": "Graph",
    "section": "Definitions:",
    "text": "Definitions:\n\nA nonlinear data structure consists of nodes connected by vertices.\nA graph can be undirected, directed, or weighted."
  },
  {
    "objectID": "posts/graph/index.html#disjoint-set",
    "href": "posts/graph/index.html#disjoint-set",
    "title": "Graph",
    "section": "Disjoint set:",
    "text": "Disjoint set:\nRepresented as a data structures in LeetCode’s terminology\nHelps to register the connectivity of a graph/network. Consists of two arrays:\n\nStore the node itself (represented by index)\nStore the parent/root vertex of the node\n\nThe problem of checking connectivity is reduced to the problem of checking whether two nodes have the same root node.\nThe disjoint set has two essential methods: find() and union(). find() : find the root node of a given vertex\n\nunion()\n\nunions two vertices and makes their root nodes the same\n\n\n\nInitialization:\n\nStart with creating an array with array[i]=i\nCan implement with emphasis on find() (Quick Find) or union() (Quick Union)\n\n\n\nQuick Find:\nInstead of storing the parent node, the array will store the root node straight-away. This makes for a quick find() but slower union() as we need to search the entire array.\n\n# UnionFind class\nclass UnionFind:\n    def __init__(self, size):\n        self.root = [i for i in range(size)]\n\n    def find(self, x):\n        return self.root[x]\n        \n    def union(self, x, y):\n        rootX = self.find(x)\n        rootY = self.find(y)\n        if rootX != rootY:\n            for i in range(len(self.root)):\n                if self.root[i] == rootY:\n                    self.root[i] = rootX\n\n    def connected(self, x, y):\n        return self.find(x) == self.find(y)\n\n\n# Test Case\nuf = UnionFind(10)\n# 1-2-5-6-7 3-8-9 4\nuf.union(1, 2)\nuf.union(2, 5)\nuf.union(5, 6)\nuf.union(6, 7)\nuf.union(3, 8)\nuf.union(8, 9)\nprint(uf.connected(1, 5))  # true\nprint(uf.connected(5, 7))  # true\nprint(uf.connected(4, 9))  # false\n# 1-2-5-6-7 3-8-9-4\nuf.union(9, 4)\nprint(uf.connected(4, 9))  # true\n\nTrue\nTrue\nFalse\nTrue\n\n\n\nTime Complexity:\n\n\n\n\nUnion-Find Constructor\nFind\nUnion\nConnected\n\n\n\n\nTime Complexity\n\\(O(N)\\)\n\\(O(1)\\)\n\\(O(N)\\)\n\\(O(1)\\)\n\n\n\nThe Union-Find Constructor is 1-liner in Python but it takes linear time to initialize in the system. #### Space Complexity: \\(O(N)\\) to store each node.\n\n\n\nQuick Union:\nunion() stops at setting the root node of the child to the root node of the parent only. find() and connected() will need to traverse more to find the answer. The worst-case time complexity for each operation swells to \\(O(N)\\), but the overall efficiency goes up.\n\n# UnionFind class\nclass UnionFind:\n    def __init__(self, size):\n        self.root = [i for i in range(size)]\n\n    def find(self, x):\n        while x != self.root[x]:\n            x = self.root[x]\n        return x\n        \n    def union(self, x, y):\n        rootX = self.find(x)\n        rootY = self.find(y)\n        if rootX != rootY:\n            self.root[rootY] = rootX\n\n    def connected(self, x, y):\n        return self.find(x) == self.find(y)\n\n\n# Test Case\nuf = UnionFind(10)\n# 1-2-5-6-7 3-8-9 4\nuf.union(1, 2)\nuf.union(2, 5)\nuf.union(5, 6)\nuf.union(6, 7)\nuf.union(3, 8)\nuf.union(8, 9)\nprint(uf.connected(1, 5))  # true\nprint(uf.connected(5, 7))  # true\nprint(uf.connected(4, 9))  # false\n# 1-2-5-6-7 3-8-9-4\nuf.union(9, 4)\nprint(uf.connected(4, 9))  # true\n\nTrue\nTrue\nFalse\nTrue\n\n\n\nTime Complexity:\n\n\n\n\nUnion-Find Constructor\nFind\nUnion\nConnected\n\n\n\n\nTime Complexity\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\\(O(N)\\)\n\n\n\nThe Union-Find Constructor is 1-liner in Python but it takes linear time to initialize in the system.\nNumber of operations to get to the root vertex will be H, where H is the height of the tree. In the worst case, H = N if the graph is a linked list.\n\n\nSpace Complexity:\n\\(O(N)\\) to store each node.\n\n\n\nOptimized Quick Union - Union by rank:\nFirst, observe that the smaller the height of the tree, the more efficient the algorithms run. So we will want to balance the tree as much as we can, but not to try too hard. One way to do so is union by rank. Rank here refers to the height of the vertex. When we union two vertices, instead of just picking the root node of one, we choose the root node of the vertex with a larger “rank”. We will merge the shorter tree under the taller tree and assign the root node of the taller tree as the root node for both vertices.\nThis is an optimization for Quick Union. To implement this, we need to make 2 changes:\n\nAdding an array rank to keep track of the rank of each vertex.\nModify the codes of union(). It is easy for the case of unequal rank. For the case of equal rank, we will assign one to the other (say, set root of Y to root of X) and increase the height of the other vertex (X’s height += 1).\n\n\n# UnionFind class\nclass UnionFind:\n    def __init__(self, size):\n        self.root = [i for i in range(size)]\n        self.rank = [1] * size\n\n    def find(self, x):\n        while x != self.root[x]:\n            x = self.root[x]\n        return x\n        \n    def union(self, x, y):\n        rootX = self.find(x)\n        rootY = self.find(y)\n        if rootX != rootY:\n            if self.rank[rootX] > self.rank[rootY]:\n                self.root[rootY] = rootX\n            elif self.rank[rootX] < self.rank[rootY]:\n                self.root[rootX] = rootY\n            else:\n                self.root[rootY] = rootX\n                self.rank[rootX] += 1\n\n    def connected(self, x, y):\n        return self.find(x) == self.find(y)\n\n\n# Test Case\nuf = UnionFind(10)\n# 1-2-5-6-7 3-8-9 4\nuf.union(1, 2)\nuf.union(2, 5)\nuf.union(5, 6)\nuf.union(6, 7)\nuf.union(3, 8)\nuf.union(8, 9)\nprint(uf.connected(1, 5))  # true\nprint(uf.connected(5, 7))  # true\nprint(uf.connected(4, 9))  # false\n# 1-2-5-6-7 3-8-9-4\nuf.union(9, 4)\nprint(uf.connected(4, 9))  # true\n\nTrue\nTrue\nFalse\nTrue\n\n\n\nTime Complexity:\n\n\n\n\nUnion-Find Constructor\nFind\nUnion\nConnected\n\n\n\n\nTime Complexity\n\\(O(N)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n\n\nThe Union-Find Constructor is 1-liner in Python but it takes linear time to initialize in the system.\nTree height will be at most log(N) + 1 when we repeatedly union components of equal rank, so find() will take O(logN) in the case.\nunion() and connected() depends on find(), so they need O(logN) as well.\n\n\n\nSpace Complexity:\n\\(O(N)\\) to store the value and the rank of each node.\n\n\n\nOptimized Quick Find - Path Compression:\nAfter finding the root node, we update the parent node of all traversed elements to their root node. We implement this with recursion.\n\n# UnionFind class\nclass UnionFind:\n    def __init__(self, size):\n        self.root = [i for i in range(size)]\n\n    def find(self, x):\n        if x == self.root[x]:\n            return x\n        self.root[x] = self.find(self.root[x])\n        return self.root[x]\n        \n    def union(self, x, y):\n        rootX = self.find(x)\n        rootY = self.find(y)\n        if rootX != rootY:\n            self.root[rootY] = rootX\n\n    def connected(self, x, y):\n        return self.find(x) == self.find(y)\n\n\n# Test Case\nuf = UnionFind(10)\n# 1-2-5-6-7 3-8-9 4\nuf.union(1, 2)\nuf.union(2, 5)\nuf.union(5, 6)\nuf.union(6, 7)\nuf.union(3, 8)\nuf.union(8, 9)\nprint(uf.connected(1, 5))  # true\nprint(uf.connected(5, 7))  # true\nprint(uf.connected(4, 9))  # false\n# 1-2-5-6-7 3-8-9-4\nuf.union(9, 4)\nprint(uf.connected(4, 9))  # true\n\nTrue\nTrue\nFalse\nTrue\n\n\n\nTime Complexity:\n\n\n\n\nUnion-Find Constructor\nFind\nUnion\nConnected\n\n\n\n\nTime Complexity\n\\(O(N)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\\(O(logN)\\)\n\n\n\n\nThe Union-Find Constructor is 1-liner in Python but it takes linear time to initialize in the system.\nThe worst-case call of find() will take O(N), but it will take O(1) afterwards. Average out, it takes O(logN) time to perform.\nunion() and connected() depends on find(), so they need O(logN) as well.\n\n\n\nSpace Complexity:\n\\(O(N)\\) to store the value and the rank of each node.\n\n\n\nFinal disjoint set with Path Compression and Union by Rank:\n\n# UnionFind class\nclass UnionFind:\n    def __init__(self, size) -> None:\n        \"\"\"\n        Function to initialize the UnionFind class\n\n        :param size: the number of nodes in the set\n        :return: None\n        \"\"\"\n        self.root = [i for i in range(size)]\n        self.rank = [1]*size\n    \n    def find(self, node):\n        \"\"\"\n        Function to return the root of a node. Recursively\n        modify the parent node if it is not yet the root\n\n        :param node: the node to search root for\\\n        :return: the root node\n        \"\"\"\n        if node == self.root[node]:\n            return node\n        # Parent node is not itself and possibly not the\n        # root node\n\n        # Set the root of node to the root of the parent node\n        # of node\n        self.root[node] = self.find(self.root[node])\n        return self.root[node]\n\n    def union(self, node1, node2):\n        root1 = self.find(node1)\n        root2 = self.find(node2)\n        # Do nothing if the two nodes are already connected\n        # Set the root of both nodes to the root of the node\n        # with heigher rank (height)\n        # In case of equal rank, set to the root of one node\n        # and increase the rank of that node\n        if root1 != root2:\n            if self.rank[root1] > self.rank[root2]:\n                self.root[root2] = root1\n            elif self.rank[root1] < self.rank[root2]:\n                self.root[root1] = root2\n            else:\n                self.root[root2] = root1\n                self.rank[root1] += 1\n    \n    def connected(self, node1, node2):\n        return self.find(node1) == self.find(node2)\n            \n\n# Test Case\nuf = UnionFind(10)\n# 1-2-5-6-7 3-8-9 4\nuf.union(1, 2)\nuf.union(2, 5)\nuf.union(5, 6)\nuf.union(6, 7)\nuf.union(3, 8)\nuf.union(8, 9)\nprint(uf.connected(1, 5))  # true\nprint(uf.connected(5, 7))  # true\nprint(uf.connected(4, 9))  # false\n# 1-2-5-6-7 3-8-9-4\nuf.union(9, 4)\nprint(uf.connected(4, 9))  # true\n\nTrue\nTrue\nFalse\nTrue\n\n\n\nTime Complexity:\n\n\n\n\n\n\n\n\n\n\n\nUnion-Find Constructor\nFind\nUnion\nConnected\n\n\n\n\nTime Complexity\n\\(O(N)\\)\n\\(O(\\alpha(N))\\)\n\\(O(\\alpha(N))\\)\n\\(O(\\alpha(N))\\)\n\n\n\n\\(\\alpha(N)\\) is the inverse Ackermann function, which is \\(O(1)\\) on average.\n\n\nSpace Complexity:\n\\(O(N)\\) to store the value and the rank of each node.\n\n\n\nProblems\n\n1. Number of provinces\nThe basic problem applying the disjoint set approach. A province is essentially a network, and two provinces are two disconnected networks. We just need to keep track of the number of provinces when we perform union.\n# UnionFind class\nclass UnionFind:\n    def __init__(self, size):\n        self.root = [i for i in range(size)]\n        # Use a rank array to record the height of each vertex, i.e., the \"rank\" of each vertex.\n        # The initial \"rank\" of each vertex is 1, because each of them is\n        # a standalone vertex with no connection to other vertices.\n        self.rank = [1] * size\n        self.count = size\n\n    # The find function here is the same as that in the disjoint set with path compression.\n    def find(self, x):\n        if x == self.root[x]:\n            return x\n        self.root[x] = self.find(self.root[x])\n        return self.root[x]\n\n    # The union function with union by rank\n    def union(self, x, y):\n        rootX = self.find(x)\n        rootY = self.find(y)\n        if rootX != rootY:\n            if self.rank[rootX] > self.rank[rootY]:\n                self.root[rootY] = rootX\n            elif self.rank[rootX] < self.rank[rootY]:\n                self.root[rootX] = rootY\n            else:\n                self.root[rootY] = rootX\n                self.rank[rootX] += 1\n            self.count -= 1\n\n    def getCount(self):\n        return self.count\n# The base problem for the whole class\n# Number of provinces are the number of disjoint sets left after performing all unions.\n# To achieve the goal, in addition to performing union-find, one also needs to take \n# care of the counting, which could be easily handled by deducting the number of nodes\n# after every union. \nclass Solution:\n    def findCircleNum(self, isConnected: list[list[int]]) -> int:\n        if not isConnected or len(isConnected) == 0:\n            return 0\n        n = len(isConnected)\n        uf = UnionFind(n)\n        for row in range(n):\n            for col in range(row + 1, n):\n                if isConnected[row][col] == 1:\n                    uf.union(row, col)\n        return uf.getCount()\n\n\n2. Number of Connected Components in an Undirected Graph\nEssentially the Number of Provinces problem. For this one, I tried not defining a separate UnionFind class but just the find() and union() functions.\nclass Solution:\n    def countComponents(self, n: int, edges: List[List[int]]) -> int:\n        rank = [1] * n\n        root = [i for i in range(n)]\n        \n        def find(x):\n            if x == root[x]:\n                return x\n            root[x] = find(root[x])\n            return root[x]\n        \n        def union(x, y):\n            # Declare n nonlocal to update n right in the union function\n            nonlocal n\n            rootX = find(x)\n            rootY = find(y)\n            if rootX != rootY:\n                if rank[rootX] > rank[rootY]:\n                    root[rootY] = rootX\n                elif rank[rootX] < rank[rootY]:\n                    root[rootX] = rootY\n                else:\n                    root[rootY] = rootX\n                    rank[rootX] += 1\n                n -= 1\n        \n        for edge in edges:\n            union(edge[0], edge[1])\n        \n        return n\n\n\n3. Graph Valid Tree\nA tree means that:\n\nNumber of edges provided must be at least the number of nodes - 1\nThere is no cycle.\n\nWe will use properties 1. to quickly rule out cases and 2. to implement the general algorithm. Property 2. is satisfied if we do not encounter two nodes with the same root when we perform the union operation; when we encounter such a case, we can return False immediately.\nclass Solution:\n    def validTree(self, n: int, edges: list[list[int]]) -> bool:\n        # write your code here\n        # Check the number of provinces\n        # False if more than 1\n        if len(edges) < n - 1:\n            return False\n        uf = UnionFind(n)\n        for A, B in edges:\n            if not uf.union(A, B):\n                return False\n        return True\n\n\n4. The Earliest Moment When Everyone Become Friends\nA.k.a the earliest moment the number of provinces goes to 1. If after performing all union operations, the number of provinces is larger than 1, we return -1\nfrom typing import List\n\nclass Solution:\n    def earliestAcq(self, logs: List[List[int]], n: int) -> int:\n        logs.sort()\n        root = [i for i in range(n)]\n        rank = [1] * n\n\n        def find(x):\n            if x == root[x]:\n                return x\n            root[x] = find(root[x])\n            return root[x]\n        \n        def union(x, y):\n            nonlocal n\n            rootX = find(x)\n            rootY = find(y)\n            if rootX != rootY:\n                if rank[rootX] > rank[rootY]:\n                    root[rootY] = rootX\n                elif rank[rootX] < rank[rootY]:\n                    root[rootX] = rootY\n                else:\n                    root[rootY] = rootX\n                    rank[rootX] += 1\n                n -= 1\n        \n        for timestamp, x, y in logs:\n            union(x, y)\n            if n == 1:\n                return timestamp\n                \n        return -1\n\n\n5. Smallest string with swaps:\nThe intuition here is recognizing that when the connected nindices form a graph that enable the corresponding characters to move around at will. THis means that we need to partition the indices into disjoint sets and sort within each set before joining them together to form the result.\nclass Solution:\n    def smallestStringWithSwaps(self, s: str, pairs: List[List[int]]) -> str:\n        length = component = len(s)\n        root = [i for i in range(length)]\n        rank = [1] * length\n\n        def find(x):\n            if x == root[x]:\n                return x\n            root[x] = find(root[x])\n            return root[x]\n        \n        def union(x, y):\n            nonlocal component\n            rootX = find(x)\n            rootY = find(y)\n            if rootX != rootY:\n                if rank[rootX] > rank[rootY]:\n                    root[rootY] = rootX\n                elif rank[rootX] < rank[rootY]:\n                    root[rootX] = rootY\n                else:\n                    root[rootY] = rootX\n                    rank[rootX] += 1\n                component -= 1\n        for pair in pairs:\n            union(pair[0], pair[1])\n        \n        # Easy case where every character is connected\n        if component == 1:\n            return ''.join(sorted(s))\n        \n        # General case\n        # We can only swap the the connected characters\n        listS = list(s)\n        copyListS = listS[:]\n        rootDict = {}\n        for i in range(length):\n            rootI = find(i)\n            if rootI in rootDict:\n                rootDict[rootI].append(i) # Ensure the nodes assoc. with a key is already sorted\n            else:\n                rootDict[rootI] = [i]\n\n        for connections in rootDict.values():\n            sortedConnections = sorted(connections, key = lambda x:ord(s[x]))\n            for i in range(len(connections)):\n                copyListS[connections[i]] = listS[sortedConnections[i]] \n        \n        return ''.join(copyListS)"
  },
  {
    "objectID": "posts/linked_list/index.html",
    "href": "posts/linked_list/index.html",
    "title": "Linked List",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/linked_list/index.html#definitions",
    "href": "posts/linked_list/index.html#definitions",
    "title": "Linked List",
    "section": "Definitions:",
    "text": "Definitions:\n\nA nonlinear data structure consists of nodes with pointers to the next nodes.\nA linked list can be singly-linked, or doubly-linked, with just head pointer or together with tail pointer.\nFor LeetCode, a singly-linked list with head pointer is usually given.\n\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\nMay fall under many patterns, such as fast and slow pointers."
  },
  {
    "objectID": "posts/linked_list/index.html#general-trick",
    "href": "posts/linked_list/index.html#general-trick",
    "title": "Linked List",
    "section": "General trick:",
    "text": "General trick:\n\n1. Sentinel head (also tail):\nWe create a dummy head first, modify everything after, and then return the actual head with dummy.next\ndummy = ListNode(None)\nhead = dummy\n# Do a lot of stuff with head\nreturn dummy.next\nThis is useful as we can use the head as a pointer to traverse the linked list while still need to return the head of the linked list in result."
  },
  {
    "objectID": "posts/linked_list/index.html#problems",
    "href": "posts/linked_list/index.html#problems",
    "title": "Linked List",
    "section": "Problems:",
    "text": "Problems:\n\n1. Reverse Linked List\n\nIntuition\n\nThis is what I call a basic problem - a problem that can be become a sub-problem for a bigger task in the future LeetCode problems. This is like a formula that you have no choice but to remember (coding interview is still closed-book at the moment, year 2023), and then exploit it over and over again.\n\n\nApproach\n\nWhen dealing with Linked List, the king is pointer - something as dangerous as pointing gun to your headan object that points to the particular position of a node in the list. The number of pointers a problem requires depend on the amount of information we need at when processing each node in the list. Here, we need to know 3 pieces: the current node (obviously), the previous node to point the current node to, and the next node to move the pointer. Hence, we will use three pointers - pre, cur, and nex to keep track while traversing the list.\n\n\nComplexity\n\nTime complexity: \\(O(n)\\): Traversing the whole list once. \nSpace complexity: \\(O(1)\\): Pointers are essentially integers, taking constant memory \n\n\n\nCode\nclass Solution:\n    def reverseList(self, head: Optional[ListNode]) -> Optional[ListNode]:\n        # Easy case:\n        if head is None or head.next is None:\n            return head\n        \n        # General case:\n        pre, cur, nex = None, head, head.next\n        while nex:\n            cur.next = pre\n            pre = cur\n            cur = nex\n            nex = nex.next\n        cur.next = pre\n        return cur\n\n\n\n2. Merge Two Sorted Lists\nChoose a new linked list. Move the two head pointers down the two lists, compare the nodes, and splice them accordingly.\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\nclass Solution:\n    def mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -> Optional[ListNode]:\n        # Easy case\n        if list1 is None:\n            return list2\n        if list2 is None:\n            return list1\n        \n        # General case\n        # Most straightforward way is to create a new linked list\n        # Utilize the sentinel head trick here\n        dummy = ListNode()\n        head = dummy\n\n        # One list will reach the end before the other, so it is divided into 2 steps:\n        # 1. Compare and splice the nodes in the two list\n        # 2. Add the rest of one list to the end of the result.\n        while list1 and list2:\n            if list1.val <= list2.val:\n                head.next = list1\n                list1 = list1.next\n            else:\n                head.next = list2\n                list2 = list2.next\n            head = head.next\n        \n        if list1:\n            head.next = list1\n        elif list2:\n            head.next = list2\n        \n        return dummy.next\n\n\n\n3. Linked List Cycle\nThe introductory fast and slow pointers problem. Because there is a loop, the fast and slow pointers are bound to meet with each other somewhere.\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\nclass Solution:\n    def hasCycle(self, head: Optional[ListNode]) -> bool:\n        # Easy case\n        if head is None or head.next is None:\n            return False\n        \n        # General case\n        fast = slow = head\n        while fast.next and fast.next.next:\n            fast = fast.next.next\n            slow = slow.next\n            if fast == slow:\n                return True\n        return False\n\n\n\n4. Remove Nth Node From End of List\nThe idea is to initialize two fast and slow pointers, but started at different positions while moving at the same speed. We want that when the fast pointer hits the end, the slow pointer is at the node before the to-be-removed node to rearrange the connection.\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\nclass Solution:\n    def removeNthFromEnd(self, head: Optional[ListNode], n: int) -> Optional[ListNode]:\n        # Easy case\n        if head.next is None:\n            return\n        \n        # General case\n        # Fast and slow pointer\n        slow = fast = head\n        for _ in range(n+1):\n            if fast:\n                fast = fast.next\n            else:\n            # If fast pointer is already None before the end, there is just one\n            # case and it is the head needs removing, so we return the next element\n                return head.next\n        # If not, we start to move the slow and fast pointers to the end.\n        while fast:\n            slow, fast = slow.next, fast.next\n        \n        slow.next = slow.next.next\n        \n        return head\n\n\n\n5. Reorder List\nThe task can be divided into 3 parts:\n\nDivide the original list into 2.\nReverse the second half.\nMerge the two halves.\n\nHere we saw for the first time all the problems above come together into one problem. Here you see that I turned the second part into another hidden method within the Solution class. This is one way to do it. Another is to define the function within the function (nested function), which you will see many Python LeetCoders do. These two methods are equivalent in solving LeetCode problems. For outside world, I think that my current method is more appreciated, as of here.\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\nclass Solution:\n    def _reverseList(self, head: ListNode):\n        pre, cur, nex = None, head, head.next\n        while nex:\n            cur.next = pre\n            pre = cur\n            cur = nex\n            nex = nex.next\n        cur.next = pre\n        return cur\n    def reorderList(self, head: Optional[ListNode]) -> None:\n        \"\"\"\n        Do not return anything, modify head in-place instead.\n        \"\"\"\n        # Easy case:\n        if head.next is None or head.next.next is None:\n            return\n        \n        # General case:\n        fast = slow = temp1 = temp2 = head\n        while fast.next and fast.next.next:\n            fast = fast.next.next\n            slow = slow.next\n        temp2 = slow.next\n        slow.next = None\n        del slow, fast\n        temp2 = self._reverseList(temp2)\n        nex1, nex2 = temp1.next, temp2.next\n        while nex2:\n            temp1.next = temp2\n            temp2.next = nex1\n            temp1, temp2 = nex1, nex2\n            nex1, nex2 = nex1.next, nex2.next\n        temp1.next = temp2\n        if nex1:\n            temp2.next = nex1\n\n\n\n6. Find the Duplicate Number\nThis is not a Linked List question, but it utilizes the fast and slow pointers to turn it into a linked list question in the subtlest way possible. As noted in the editorial, this is similar to Linked List Cycle II. In the original one, fast and slow pointer is used to detect the cycle. Now the objective is to return the cycle entrance. Fast and slow pointers can be proved mathematically to work for that problem, by having the fast pointer starts at the intersection and the slow pointer starts at the start, they will meet each other at the entrance!\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\nclass Solution:\n    def findDuplicate(self, nums: List[int]) -> int:\n        # Phase 1: Intersection point\n        fast = slow = nums[0]\n        while True:\n            fast = nums[nums[fast]]\n            slow = nums[slow]\n            if fast == slow:\n                break\n        \n        # Phase 2: Find the cycle entrance\n        slow = nums[0]\n        while slow != fast:\n            fast = nums[fast]\n            slow = nums[slow]\n        \n        return slow\n\n\n\n7. Copy List with Random Pointer\nSometimes overthinking is a big problem. This question is super hard - if you try to do it in one go. The secret is to not do so. You do it in 2 passes. And use a HashMap to keep track of the copy.\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\n\"\"\"\n# Definition for a Node.\nclass Node:\n    def __init__(self, x: int, next: 'Node' = None, random: 'Node' = None):\n        self.val = int(x)\n        self.next = next\n        self.random = random\n\"\"\"\n\nclass Solution:\n    def copyRandomList(self, head: 'Optional[Node]') -> 'Optional[Node]':\n        # The first key in the dictionary to deal with the pointer to \n        # None of a node.\n        oldToCopy = {None: None}\n\n        # 1st pass: Create all the nodes only\n        cur = head\n        while cur:\n            oldToCopy[cur] = Node(cur.val)\n            cur = cur.next\n        \n        # 2nd pass: Create the links within the list\n        cur = head\n        while cur:\n            copy = oldToCopy[cur]\n            copy.next = oldToCopy[cur.next]\n            copy.random = oldToCopy[cur.random]\n            cur = cur.next\n        \n        return oldToCopy[head]\n\n\n\n8. Palindrome Linked List\nHere we see the reverse linked list sub-problem becomes relevant again. The algorithm is:\n\nGo to the middle of the linked list and divide it in 2 halves.\nReverse the second half.\nCheck each pair of nodes of two halves and return result.\n\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\nclass Solution:\n    def _reverseList(self, head: Optional[ListNode]) -> Optional[ListNode]:\n        pre, cur, nex = None, head, head.next\n        while nex:\n            cur.next = pre\n            pre = cur\n            cur = nex\n            nex = nex.next\n        cur.next = pre\n        return cur\n\n    def isPalindrome(self, head: Optional[ListNode]) -> bool:\n        # Easy case:\n        if head is None or head.next is None:\n            return True\n        \n        # General case:\n        slow, fast = head, head.next\n        while fast.next and fast.next.next:\n            slow = slow.next\n            fast = fast.next.next\n        if not fast.next:\n            temp = slow.next\n            slow.next = None\n        else:\n            temp = slow.next.next\n            slow.next.next = None\n        temp = self._reverseList(temp)\n        while temp:\n            if temp.val != head.val:\n                return False\n            temp, head = temp.next, head.next\n        return True\n\n\n\n9. Add Two Numbers\nThe second problem added to LeetCode, after Two Sum. The digits are arranged in reverse order, which makes it easier to add them. The algorithm can be represented as:\n\nWhile both heads not yet hit the end, traverse and add each pair of digit (remember to log the remaining).\nWhen a head hits the end, check if the remaining list has any node left and add them to the result.\n\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, val=0, next=None):\n#         self.val = val\n#         self.next = next\nclass Solution:\n    def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -> Optional[ListNode]:\n        dummy = ListNode()\n        head = dummy\n        remain = 0\n        while l1 and l2:\n            raw_sum = l1.val + l2.val + remain\n            remain = raw_sum // 10\n            temp_sum = raw_sum % 10\n            head.next = ListNode(temp_sum)\n            head = head.next\n            l1, l2 = l1.next, l2.next\n        \n        while l1:\n            if remain == 0:\n                head.next = l1\n                break\n            raw_sum = l1.val + remain\n            remain = raw_sum // 10\n            temp_sum = raw_sum % 10\n            head.next = ListNode(temp_sum)\n            head = head.next\n            l1 = l1.next\n        \n        while l2:\n            if remain == 0:\n                head.next = l2\n                break\n            raw_sum = l2.val + remain\n            remain = raw_sum // 10\n            temp_sum = raw_sum % 10\n            head.next = ListNode(temp_sum)\n            head = head.next\n            l2 = l2.next\n        if remain != 0:\n            head.next = ListNode(remain)\n        return dummy.next\n\n\n\n10. LRU Cache\n\nIntuition\n\n\n\nApproach\n\n\n\nComplexity\n\nTime complexity: \nSpace complexity: \n\n\n\nCode"
  },
  {
    "objectID": "posts/sliding_window/index.html",
    "href": "posts/sliding_window/index.html",
    "title": "Sliding window",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/sliding_window/index.html#definitions",
    "href": "posts/sliding_window/index.html#definitions",
    "title": "Sliding window",
    "section": "Definitions",
    "text": "Definitions\nSliding window at its core is processing the data in an array in chunk, with limits set by 2 pointers at both sides. Sliding window is particularly suitable if we need to performf repeated operations on a sub-array. Instead of going crazy over all of the possible chunks, we can just change the result (e.g. sum of elements in the window) by discarding the leaving and accounting for the entering element(s)."
  },
  {
    "objectID": "posts/sliding_window/index.html#problems",
    "href": "posts/sliding_window/index.html#problems",
    "title": "Sliding window",
    "section": "Problems",
    "text": "Problems\n\n1. Best Time to Buy and Sell Stock\n\nIntuition\n\nThe first problem in sliding window. The brute-force way of doing this is to check every single pair fo number for the largest right-left pair. The time complexity will be \\(O(n^2)\\). How do we modify this solution to be easier?\nFirst, the brute-force algorithm involves keeping track of the maximum number so far. This is a good thing that we can keep.\nSecond, let’s assume this array of [2, ..., 1]. Let’s say that the maximum of the subarray between 2 and 1 is 6: [2, ...6..., 1] (it does not matter where the 6 is as long as it occurs before 1). The max profit we can get so far is \\(6-2=4\\). To increase the profit, there are two ways: shift the buy date to a day with smaller price, or shift the sell date to a day with a larger price. Let’s say there is a 7 after 1. We can increase the profit if we shift the sell date from 6 to 7: [2, ...6..., 1, ...7...]. However, the best profit is when we shift the buy date from 2 to 1 as well. Now, because we already keep track of the max profit so far, so it cannot be worse if we shift the buy date every single time we find a better buy date.\n\n\nApproach\n\nThe algorithm: 1. Initialize the left and right sides of the window + the max profit so far. 2. Traverse the array with the right side. 3. If we encounter a better buy date (right < left), update the buy date. If we encounter a potential sell date (right >= left), check and update the profit so far. 4. Return the max profit at the end. #### Complexity - Time complexity:  \\(O(n)\\): We are traversing the array once. - Space complexity:  \\(O(1)\\): We are only keeping track of a bunch of integers. #### Code\nclass Solution:\n    def maxProfit(self, prices: List[int]) -> int:\n        buyDay = maxProfit = 0\n        for sellDay in range(len(prices)):\n            if prices[sellDay] < prices[buyDay]:\n                buyDay = sellDay\n            else:\n                currentProfit = prices[sellDay] - prices[buyDay]\n                if currentProfit > maxProfit:\n                    maxProfit = currentProfit\n        return maxProfit\n\n\n\n2. Maximum Subarray:\n\nIntuition\n\nThis is one problem in the class of “if you know, you know”, though of course you can discover for yourself from keen observation.\nThe named algorithm is Kadane’s algorithm. I did not know the exact formal definition for it, but its pattern is that of sliding window (though the class will be dynamic programming). The intuition here is that if we keep track of the sum of a subarray, one that sum reaches a negative number, the whole subarray can be discarded from consideration, because whatever the value of the next element, the subarray sum will always be worse if we include the negative-sum subarray. Incredible intuition, though I could not see this in the first time I did it.\n\n\nApproach\n\n\nTraverse the array, keeping track of the current sum curSum and the maximum sum so far maxSum of the array.\nAt each element, if curSum becomes negative, we reset it to 0 first. Then we add the element to curSum. If curSum becomes larger than maxSum, we update maxSum.\nIf we reach the end of the array, return maxSum.\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): Traversing the whole array once.\nSpace complexity:  \\(O(1)\\): We just need to keep track of a bunch of integers.\n\n\n\nCode\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -> int:\n        curSum, maxSum = 0, float('-inf')\n        for num in nums:\n            if curSum < 0:\n                curSum = 0\n            curSum += num\n            if curSum > maxSum:\n                maxSum = curSum\n        return maxSum"
  },
  {
    "objectID": "posts/stack/index.html",
    "href": "posts/stack/index.html",
    "title": "Stack",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/stack/index.html#definitions",
    "href": "posts/stack/index.html#definitions",
    "title": "Stack",
    "section": "Definitions:",
    "text": "Definitions:\nA data structure with the trademark of LIFO (last in, first out). It has direct analogy to the list in Python, and we actually will use the built-in list as the stack. There are alternatives to have a stack (such as queue.LifoQueue), but normally, a python list suffices."
  },
  {
    "objectID": "posts/stack/index.html#problems",
    "href": "posts/stack/index.html#problems",
    "title": "Stack",
    "section": "Problems:",
    "text": "Problems:\n\n1. Valid Parentheses\n\nIntuition\n\nThis is the Two Sum of stack. The problem requirements can be translated into the intuition that if we push the left parenthesis into a stack and pop them out whenever we encounter a right parenthesis, the correct parenthesis sequence will generate all matching pairs.\n\n\nApproach\n\n\nInitialize, including a HashMap for right and left parenthesis, and check that the array has an even number of elements.\nTraverse the array of parentheses.\nIf we encounter a left parenthesis, push it to a stack.\nIf we encounter a right parenthesis, compare it to the one we pop from the stack. If they do not match, return False immediately.\nAt the end, return True if the stack is empty, else False.\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): Traversing the whole array once.\nSpace complexity:  \\(O(n)\\): At the worst case, we will need to store half of the array inside the stack.\n\n\n\nCode\nclass Solution:\n    def isValid(self, s: str) -> bool:\n        if len(s) % 2:\n            return False\n        stack = []\n        parenthesisDict = {'{':'}', '[':']', '(':')'}\n        for parenthesis in s:\n            if parenthesis in parenthesisDict:\n                stack.append(parenthesis)\n            else:\n                if len(stack) == 0 or parenthesis != parenthesisDict[stack.pop()]:\n                    return False\n        return len(stack) == 0\n\n\n\n2."
  },
  {
    "objectID": "posts/tree/index.html",
    "href": "posts/tree/index.html",
    "title": "Tree",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/tree/index.html#definitions",
    "href": "posts/tree/index.html#definitions",
    "title": "Tree",
    "section": "Definitions",
    "text": "Definitions\nThe upgraded version of a linked list.\n\nIt is acyclic (doesn’t contain any cycles);\nThere exists a path from the root to any node;\nHas \\(N - 1\\) edges, where \\(N\\) is the number of nodes in the tree; and\nEach node has exactly one parent node with the exception of the root node.\n\nFor binary tree, all nodes have at most 2 children.\n# Definition for a binary tree node.\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\n\n\n\n\n\n\nTerms\nMeaning\n\n\n\n\nNode & Edges\nTrivia\n\n\nRoot\nThe first node\n\n\nLeaf node\nNode with no child\n\n\nInternal node\nNode with at least one child\n\n\nAncestor\nNodes that are between the pathfrom the root to the current root. Including the node itself\n\n\nDescendent\nNodes that are between the pathfrom the root to the current root. Including the node itself\n\n\nLevel\nNumber of ancestors from that nodeuntil the root node. Start at 0 or 1, go down.\n\n\nHeight\nNumber of edges on the longest path fromthat node to a leaf. Start at 0, go up.\n\n\nDepth\nNumber of edges on the path from rootto that node. Start at 0, go down."
  },
  {
    "objectID": "posts/tree/index.html#categories",
    "href": "posts/tree/index.html#categories",
    "title": "Tree",
    "section": "Categories",
    "text": "Categories\n\nFull binary tree\n\nEvery node has 0 or 2 children.\n\nComplete binary tree\n\nAll levels are completely filled except possibly the last level. All nodes are as far left as possible.\n\nPerfect binary tree\n\nAll internal nodes have two children and all leaf nodes have the same level\n\nBalanced binary tree\n\nEvery node fulfil the condition: height difference of the left and right subtree of the node is not more than than 1. Searching, insertion, and deletion in a balanced binary tree takes \\(O(logn)\\) instead of \\(O(n)\\) in an unbalanced binary tree."
  },
  {
    "objectID": "posts/tree/index.html#notes",
    "href": "posts/tree/index.html#notes",
    "title": "Tree",
    "section": "Notes:",
    "text": "Notes:\n\n1. Depth-first search\nDepth-first search is the first heavily used technique. It is essentially pre-order traversal of a tree. All traversal types are given here:\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\n    def pre_order_traversal(self, root: TreeNode):\n        if root is not None:\n            print(root.val)\n            pre_order_traversal(root.left)\n            pre_order_traversal(root.right)\n\n    def in_order_traversal(self, root: TreeNode):\n        if root is not None:\n            in_order_traversal(root.left)\n            print(root.val)\n            in_order_traversal(root.right)\n    \n    def post_order_traversal(self, root: TreeNode):\n        if root is not None:\n            post_order_traversal(root.left)\n            post_order_traversal(root.right)\n            print(root.val)\nDepth-first search is often implemented in recursion. In thinking in recursion, the most important thing is visualization of the call stack.\nIn thinking in recursion, one must forget the whole picture and start thinking about each node. For each node, decide how the information there should be processed, then recurse on the children. When you are a node, the only thing you know are 1. node value and 2. how to get to children. The recursive function should manipulate these things.\nIn defining the recursive functions, there are two things to decide when we define:\n\nreturn value - the value the child passes to the parent. For example, for the max depth problem this is the max depth for the current node’s subtree.\nstate - the value the parent passes to the child. For example, to know if the current node’s value is larger than its parent we have to maintain the parent’s value as a state.\n\nAnother way to solve the problem is to replace return value with a global variable."
  },
  {
    "objectID": "posts/tree/index.html#problems",
    "href": "posts/tree/index.html#problems",
    "title": "Tree",
    "section": "Problems",
    "text": "Problems\n\n1. Same Tree\n\nIntuition\n\nThe first application of recursion. Thinking in node, the two are the same if each pair of nodes has the same value. Therefore, the recursive function should 1. check the current pair then 2. check the two child pairs. This problem is simple enough so that we don’t need to write another helper function to deal with the situation.\n\n\nApproach\n\n\nCreate the recursive function. The base case is two roots are equal if both are None, else False.\nCall the recursive function for the child before returning the result.\n\n\n\nComplexity\nWith \\(n\\) the number of nodes in the tree.\n\nTime complexity:  \\(O(n)\\): We need to visit every node once.\nSpace complexity:  \\(O(log(n))\\): This is the average height of the recursive call stack. In the worst case of a total unbalance tree where all nodes are grouped to 1 side, the space complexity is \\(O(n)\\).\n\n\n\nCode\nclass Solution:\n    def isSameTree(self, p: Optional[TreeNode], q: Optional[TreeNode]) -> bool:\n        if not p and not q:\n            return True\n        if p and q:\n            return (p.val==q.val and\n                    self.isSameTree(p.left, q.left) and\n                    self.isSameTree(p.right, q.right))\n        return False\nHowever, this problem is not just depth-first search. Breadth-first search can be used as well, as in the iterative solution.\nfrom collections import deque\nclass Solution:\n    def isSameTree(self, p, q):\n        def check(p, q):\n            # if both are None\n            if not p and not q:\n                return True\n            # one of p and q is None\n            if not q or not p:\n                return False\n            if p.val != q.val:\n                return False\n            return True\n        \n        deq = deque([(p, q),])\n        while deq:\n            p, q = deq.popleft()\n            if not check(p, q):\n                return False\n            \n            if p:\n                deq.append((p.left, q.left))\n                deq.append((p.right, q.right))\n                    \n        return True\n\n\n\n2. Flip Equivalent Binary Trees\nThe tweaked problem of above. The trees now are the same if each pair of nodes are the same and each pair of corresponding children are the sae or each pair of flipped children are the same. This means adding a case in return of the recursive function.\n\nIntuition\n\nSame as above\n\n\nApproach\n\nSame as above\n\n\nComplexity\n\nTime complexity:  Same as above\nSpace complexity:  Same as above\n\n\n\nCode\nclass Solution:\n    def flipEquiv(self, root1: Optional[TreeNode], root2: Optional[TreeNode]) -> bool:\n        if not root1 and not root2:\n            return True\n        if root1 and root2:\n            return (root1.val==root2.val and\n                    ((self.flipEquiv(root1.left, root2.left) and\n                    self.flipEquiv(root1.right, root2.right)) or\n                    (self.flipEquiv(root1.left, root2.right) and\n                    self.flipEquiv(root1.right, root2.left))))\n        return False\n\n\n\n3. Maximum Depth of Binary Tree\n\nIntuition\n\nIt is easier if we have a picture.\n\nLet’s think recursively about the problem. Let’s say we start with an empty tree. The height will immediately be 0. If we have a tree with just 1 node, the depth should be 1. Now suppose that node is a child of another node. This node will have two 2 children: the aforementioned node and an empty node. The depth of the empty child should be 0 while the depth of the non-empty child is 1. Ultimately, the depth of the root node will be 2. By imagining this for an increasingly bigger tree, we can see that the depth of a node is the maximum depth of its child node plus 1. In the recursion framework, the return value is the depth a node, which we pass up from the child to the parent. #### Approach  In terms of recursive algorithm, we have to define the base case - return 0 if the root is empty. Afterwards, we just need to call the fuction recursively.\n\n\nComplexity\nWith \\(n\\) the number of tree nodes:\n\nTime complexity:  \\(O(n)\\): We have a traverse every node in the tree.\nSpace complexity:  \\(O(log(n))\\): This is the average height of the recursive call stack. In the worst case of a total unbalance tree where all nodes are grouped to 1 side, the space complexity is \\(O(n)\\).\n\n\n\nCode\nclass Solution:\n    def maxDepth(self, root: Optional[TreeNode]) -> int:\n        if root is None:\n            return 0\n        \n        return 1 + max(self.maxDepth(root.left), self.maxDepth(root.right))\n\n\n\n4. Invert Binary Tree\n\nIntuition\n\nRecursively, the problem can be reduced to switching the left and right children of every node, from the root to the leaves.\n\n\nApproach\n\n\nCreate a recursive function with the only line: root.left, root.right = root.right, root.left\nCall the recursive function on the children after switching the left and right children. You can do that after switching as well. It makes no difference in this problem.\nReturn the switched root.\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): We need to visit every node once.\nSpace complexity:  \\(O(log(n))\\): This is the average height of the recursive call stack. In the worst case of a total unbalance tree where all nodes are grouped to 1 side, the space complexity is \\(O(n)\\).\n\n\n\nCode\nfrom typing import Optional\nclass Solution:\n    def invertTree(self, root: Optional[TreeNode]) -> Optional[TreeNode]:\n        if not root:\n            return\n        root.left, root.right = root.right, root.left\n        self.invertTree(root.left)\n        self.invertTree(root.right)\n\n        return root\n\n\n\n4. Lowest Common Ancestor of a Binary Search Tree\n\nIntuition\n\nIn a binary search tree, there are five cases when you compare the value of a node to the two values of p and q (without violating generality, we can assume that p.val < q.val).\n\nnode.val < p.val\nnode.val > q.val\np.val < node.val < q.val\nnode.val == p.val\nnode.val == q.val\n\nBased on the constraint of the binary search tree, the correct result would be the first node that have the propertyp.val <= node.val <= q.val. This is an unexpected intuition, but it is correct in a BST. The reason is p and q will be split at this node into 2 different subtrees afterwards, so no lower common ancestor can exist in the tree.\n\n\nApproach\n\n\nInitialized the nodes - small & large as the smaller and the larger between p and q. This is optional as the famous NeetCode did not do it.\nTraverse the tree with a while loop.\nMove to the right child of the node for case 1. above, and move to the left child of the node for case 2. above. If it is neither case, return the current node immediately\n\n\n\nComplexity\n\nTime complexity:  \\(O(log(n))\\): We need to traverse only one tree node per level.\nSpace complexity:  \\(O(1)\\): We just need to store two TreeNode for every case.\n\n\n\nCode\nclass Solution:\n    def lowestCommonAncestor(self, root: 'TreeNode', p: 'TreeNode', q: 'TreeNode') -> 'TreeNode':\n        temp = root\n        small = p if p.val < q.val else q\n        large = p if p != small else q\n        while temp:\n            if temp.val > large.val:\n                temp = temp.left\n            elif temp.val < small.val:\n                temp = temp.right\n            else:\n                return temp\n\n\n\n5. Balanced Binary Tree\n\nIntuition\n\n\n\nHeight-Balanced\n\nA height-balanced binary tree is a binary tree in which the depth of the two subtrees of every node never differs by more than one.\n\n\n\nFrom the definition, the height-balancedness of a binary tree depends on the height-balancedness of a node. This means that the problem can be solved recursively at the node level. Now, the height-balancedness of a node depends on the height-balancedness of its children, and the height-balancedness of itself. The main problem becomes calculating whether a node is height-balanced or not. We can determine this from the depths of the children, more specifically whether absolute difference in depths of the children is smaller than 2.\n\n\nApproach\n\nThe recursive function should be defined which return 2 values: whether a node is height-balanced & the depth of the node.\nNote that there is a way to define a recursive function to do more work than you need - by defining it top-down. In the example below is my first attempt, where I defined a function to get the depth of a node only. Subsequent calling leads to recomputation of depths of every node for each node in the tree, resulting in an \\(O(n^2)\\) time complexity.\nTo reduce the time complexity, we will use more memory by returning both the 2 values in the output. The solution will also be constructed in such a way that a depth will only be computed once - by defining it bottom-up.\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): We need to traverse and perform operations on every tree node once.\nSpace complexity:  \\(O(log(n))\\): The average maximum height of the recursive call stack.\n\n\n\nCode\n# Naive recursion - top-down approach\nclass Solution:\n    def _getDepth(self, root: Optional[TreeNode]) -> int:\n        if not root:\n            return 0\n        return 1 + max(self._getDepth(root.left), self._getDepth(root.right))\n\n    def isBalanced(self, root: Optional[TreeNode]) -> bool:\n        if not root or (not root.left and not root.right):\n            return True\n        return ((abs(self._getDepth(root.left) - self._getDepth(root.right)) < 2) &\n                (self.isBalanced(root.left)) & (self.isBalanced(root.right)))\n# Optimal recursion - bottom-up approach\nclass Solution:\n    def _isBalanced(self, root: Optional[TreeNode]) -> bool:\n        if not root:\n            return [True, 0]\n        left, right = self._isBalanced(root.left), self._isBalanced(root.right)\n        balanced = left[0] and right[0] and abs(left[1]-right[1]) < 2\n        return [balanced, 1 + max(left[1], right[1])]\n    def isBalanced(self, root: Optional[TreeNode]) -> bool:\n        return self._isBalanced(root)[0]\n\n\n\nDigression: Top-down or bottom-up recursion\nIn defining the recursive function for a problem, there are two approaches: top-down or bottom-up. That sounds dangerous, but it just means that do we start the problem at the head (root of the tree) or do we start the problem at the foot (leaf of the tree). It really depends on the situation to figure out the best way to enter a problem.\nFor Balance Binary Tree, it makes more sense to go from the leaf, as a piece of the information that we need (the depth of a node) flows from the leaf to the root. If we choose to go top-down, it will become inefficient, because we are required to reach the bottom first before going back to the top, ending up in the \\(O(n^2)\\) time complexity observed.\nIf the problem is about the height of a tree, information will flow top-down because of the definition of the height of a node.\nBut more often than not, a problem is a bottom-up one. This means that we usually need information from the children to perform certain operations at the node.\n\n\n6. Diameter of Binary Tree\n\nIntuition\n\nThis is another problem that requires the depth of each node. Math manipulation will make you realize that the diameter at a node is the sum of the depths of the children.\n\n\nApproach\n\nBottom-up! Bottom-up! The recursive function will be defined bottom-up-ly! We will nest the helper function inside the main function (is it a closure? I think so). We will keep diameter as a seperate nonlocal variable storing the largest diameter so far. The return will only be the depth of a node, as the diameter is calculated and updated within the function.\nEnclosure, or, to be safe, nested function will look confusing the first time you saw it. It is alright. The best way is to see so much that its weirdness does not bother you any more.\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): We will visit every node in the tree once.\nSpace complexity:  \\(O(log(n))\\): The average maximum height of the call stack.\n\n\n\nCode\nclass Solution:\n    def diameterOfBinaryTree(self, root: Optional[TreeNode]) -> int:\n        diameter = 0\n        def _diameterOfBinaryTree(root):\n            nonlocal diameter\n            if not root:\n                return 0\n            left, right = _diameterOfBinaryTree(root.left), _diameterOfBinaryTree(root.right)\n            diameter = max(diameter, left+right)\n            return 1 + max(left, right)\n        _diameterOfBinaryTree(root)\n        return diameter"
  },
  {
    "objectID": "posts/two_pointers/index.html",
    "href": "posts/two_pointers/index.html",
    "title": "Two pointers",
    "section": "",
    "text": "Firstly, See About page."
  },
  {
    "objectID": "posts/two_pointers/index.html#definitions",
    "href": "posts/two_pointers/index.html#definitions",
    "title": "Two pointers",
    "section": "Definitions:",
    "text": "Definitions:\nThis is not a data structure, but a pattern of coding interview questions. This is something passed down from languages with pointers such as C++ or Java. The idea is that we will traverse the array in both direction. This operation has use in some problems, particularly palindrome checking."
  },
  {
    "objectID": "posts/two_pointers/index.html#problem",
    "href": "posts/two_pointers/index.html#problem",
    "title": "Two pointers",
    "section": "Problem",
    "text": "Problem\n\n1. Valid Palindrome\n\nIntuition\n\nThe basic problem of two pointers. The problem can be solved by many ways (strip the string then reverse, etc.), but the most straightforward way if you know two pointers is using two pointers. You will traverse the string in both directions, bypassing non-alphanumeric characters. For each alphanumeric pairs, the lowercase versions of the character must match.\n\n\nApproach\n\n\nInitialize two pointers.\nTraverse the array from both ends.\nFor both pointers, if we encounter a non-alphanumeric characters, we increment or decrement further.\nIf we encounter a differing pair, return False immediately.\nAt the end, return True if the right end (or the left end, depending on which pointer is moved first) has moved, else False (this is to resolve edge cases suchas \".,\").\n\n\n\nComplexity\n\nTime complexity:  \\(O(n)\\): Traversing the whole array once.\nSpace complexity:  \\(O(1)\\): Pointers are essentially integers.\n\n\n\nCode\nclass Solution:\n    def isPalindrome(self, s: str) -> bool:\n        # Edge case\n        if len(s) < 2:\n            return True\n\n        # General case\n        leftPointer, rightPointer = 0, len(s) - 1\n        while leftPointer <= rightPointer:\n            while (leftPointer < rightPointer) and (not s[leftPointer].isalnum()):\n                leftPointer += 1\n            while (leftPointer < rightPointer) and (not s[rightPointer].isalnum()):\n                rightPointer -= 1\n            \n            if s[leftPointer].lower() != s[rightPointer].lower():\n                return False\n            leftPointer += 1\n            rightPointer -= 1\n        \n        return rightPointer < len(s) - 1"
  }
]